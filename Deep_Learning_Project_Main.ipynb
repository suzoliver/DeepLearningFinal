{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z5kN9nud711"
      },
      "source": [
        "Suzanne Oliver\n",
        "\n",
        "Deep Learning Final Project\n",
        "\n",
        "Fall 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "65ykAj7WhvQx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-pRHHq2d7Ye"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "# just the files we need, otherwise takes too long\n",
        "# easier to just download dataset and mount from google drive\n",
        "\n",
        "validSubjs = list(range(1,110))\n",
        "validFileIdx = [3,7,11] # files with hand movement\n",
        "\n",
        "for iSubj in validSubjs:\n",
        "  subjID = str(iSubj).zfill(3)\n",
        "  for iFile in validFileIdx:\n",
        "    fileID = str(iFile).zfill(2)\n",
        "    curr_url = \"https://physionet.org/files/eegmmidb/1.0.0/S\"+ subjID + \"/S\" + subjID + \"R\" + fileID +\".edf\"\n",
        "    !wget -r -N -c -np $curr_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFNvqBQOCNNV"
      },
      "outputs": [],
      "source": [
        "!pip install pyEDFlib\n",
        "import pyedflib # to read EEG files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/gumpy-bci/gumpy\n",
        "import gumpy # signal processing toolbox"
      ],
      "metadata": {
        "id": "GZ9MMKKEjFwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZKjK67ujwDF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "#import sklearn # OBLY UNCOMMENT TO INDUCE GUMPY CHANNEL FLIPPING BUG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6CJeAKdW8ct"
      },
      "outputs": [],
      "source": [
        "# list of valid subjects (per original paper)\n",
        "# remove subjects with inaccuracies in annotations\n",
        "validSubjs = list(range(1,110))\n",
        "validSubjs.remove(38)\n",
        "validSubjs.remove(88)\n",
        "validSubjs.remove(89)\n",
        "validSubjs.remove(92)\n",
        "validSubjs.remove(100)\n",
        "validSubjs.remove(104)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo_DCaAjhWpZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get all the data into an array to deal with\n",
        "\n",
        "nTrials = 360*len(validSubjs)\n",
        "\n",
        "# initialize variables\n",
        "dataset_x = np.zeros((nTrials, 64, 80))\n",
        "dataset_y = np.zeros((nTrials,2))\n",
        "\n",
        "validFileIdx = [3,7,11]\n",
        "freq = 160 # sample rate in Hz\n",
        "subLen = 80\n",
        "count = 0\n",
        "\n",
        "for iSubj in validSubjs:\n",
        "  if iSubj % 10 == 0:\n",
        "    print(iSubj)\n",
        "  subjID = str(iSubj).zfill(3)\n",
        "  for iFile in validFileIdx:\n",
        "    fileID = str(iFile).zfill(2)\n",
        "    # open relevant file\n",
        "    f = pyedflib.EdfReader(\"/content/drive/MyDrive/DL Final/files/S\"+ subjID + \"/S\" + subjID + \"R\" + fileID +\".edf\")\n",
        "    annotations = f.readAnnotations()\n",
        "    trialStartTimes = annotations[0] # get start times of each trial\n",
        "    trialTypes = annotations[2] # get trial type (rest, left or right)\n",
        "    temp = f.readSignal(1)\n",
        "    # initalize variable for raw signal\n",
        "    raw = np.zeros((64, len(temp)))\n",
        "    for iChan in range(64):\n",
        "      raw[iChan, :] = f.readSignal(iChan)\n",
        "    for iTrial in range(len(trialStartTimes)):\n",
        "      if trialTypes[iTrial] != 'T0': # only process non-rest trials\n",
        "        startInd = int(trialStartTimes[iTrial] * freq)\n",
        "        trialSig = raw[:,startInd:(startInd+(subLen*8))] # get signal for this trial\n",
        "        for iChan in range(64):\n",
        "          # for each channel, do pre-processing steps\n",
        "          notch_signal = gumpy.signal.notch(trialSig[iChan,:], 60/(160 / 2)) # notch filter\n",
        "          filt_signal = gumpy.signal.butter_bandpass(notch_signal, 2, 60, fs=160, order = 5) # bandpass filter\n",
        "          clean_signal = np.squeeze(gumpy.signal.artifact_removal(filt_signal.reshape((-1, 1)))[0]) # artifact removal\n",
        "          for iSubset in range(8):\n",
        "            # break signal into 8 subset and record label for each\n",
        "            dataset_x[count+iSubset, iChan, :] = clean_signal[(iSubset*subLen):((iSubset+1)*subLen)]\n",
        "            if trialTypes[iTrial] == 'T2':\n",
        "              dataset_y[count+iSubset,1] = 1\n",
        "            else:\n",
        "              dataset_y[count+iSubset,0] = 1\n",
        "        count += 8\n",
        "    f.close() # close file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save data to use later\n",
        "np.save('/content/drive/MyDrive/DL Final/dataset_x.npy', dataset_x)\n",
        "np.save('/content/drive/MyDrive/DL Final/dataset_y.npy', dataset_y)"
      ],
      "metadata": {
        "id": "fIZtyIFzjo9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Ready to Train"
      ],
      "metadata": {
        "id": "Pfgc06XljvRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved preprocessed data\n",
        "dataset_y = np.load('/content/drive/MyDrive/DL Final/dataset_y.npy')\n",
        "dataset_x = np.load('/content/drive/MyDrive/DL Final/dataset_x.npy')"
      ],
      "metadata": {
        "id": "NNq8zpVAkW6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M29wkPAqiWdQ"
      },
      "outputs": [],
      "source": [
        "# select trials for train (include validation data)\n",
        "\n",
        "nTrials = 360*103\n",
        "\n",
        "random.seed(25)\n",
        "train_idx = random.sample(list(range(nTrials)), int(nTrials*0.8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phRkNuYYioqs"
      },
      "outputs": [],
      "source": [
        "# build train dataset\n",
        "x_train = dataset_x[train_idx, :,:]\n",
        "y_train = dataset_y[train_idx,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH-nAyqDkEdj"
      },
      "outputs": [],
      "source": [
        "# build test dataset out of leftover data\n",
        "\n",
        "nTrials = 360*103\n",
        "test_idx = list(range(nTrials))\n",
        "\n",
        "for idx in train_idx:\n",
        "  test_idx.remove(idx)\n",
        "\n",
        "x_test = dataset_x[test_idx, :,:]\n",
        "y_test = dataset_y[test_idx,:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape x datasets to feed into model\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1], x_train.shape[2])\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2])"
      ],
      "metadata": {
        "id": "OZ5dNv_N3JtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X7Rbd7ojAr6"
      },
      "source": [
        "# Define Model and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFv7QlDJjD33"
      },
      "outputs": [],
      "source": [
        "# import keras and set data format\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "\n",
        "keras.backend.set_image_data_format('channels_first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmFHQVSdy8dR"
      },
      "outputs": [],
      "source": [
        "# Create fusionNet\n",
        "\n",
        "nChans = 64\n",
        "\n",
        "\n",
        "axVal = 1\n",
        "input_shape = (1, 64, 80)\n",
        "\n",
        "#build first branch of model\n",
        "input1 = keras.Input(shape=(input_shape))\n",
        "branch1 = keras.layers.Conv2D(4, (1, 64), padding='same', use_bias=False, input_shape=input_shape)(input1)\n",
        "branch1 = keras.layers.BatchNormalization(axis=axVal)(branch1)\n",
        "branch1 = keras.layers.DepthwiseConv2D((64,1), depth_multiplier=2, depthwise_constraint=keras.constraints.max_norm(1.), use_bias=False)(branch1)\n",
        "branch1 = keras.layers.BatchNormalization(axis=axVal)(branch1)\n",
        "branch1 = keras.layers.Activation('elu')(branch1)\n",
        "branch1 = keras.layers.AveragePooling2D((1,4))(branch1)\n",
        "branch1 = keras.layers.Dropout(0.5)(branch1)\n",
        "branch1 = keras.layers.SeparableConv2D(16, (1,8), padding='same', use_bias=False)(branch1)\n",
        "branch1 = keras.layers.BatchNormalization(axis=axVal)(branch1)\n",
        "branch1 = keras.layers.Activation('elu')(branch1)\n",
        "branch1 = keras.layers.AveragePooling2D((1,8))(branch1)\n",
        "branch1 = keras.layers.Dropout(0.5)(branch1)\n",
        "branch1 = keras.layers.Flatten()(branch1)\n",
        "\n",
        "# second branch\n",
        "input2 = keras.Input(shape=(input_shape))\n",
        "branch2 = keras.layers.Conv2D(8, (1, 128), padding='same', use_bias=False, input_shape=input_shape)(input2)\n",
        "branch2 = keras.layers.BatchNormalization(axis=axVal)(branch2)\n",
        "branch2 = keras.layers.DepthwiseConv2D((64,1), depth_multiplier=2, depthwise_constraint=keras.constraints.max_norm(1.), use_bias=False)(branch2)\n",
        "branch2 = keras.layers.BatchNormalization(axis=axVal)(branch2)\n",
        "branch2 = keras.layers.Activation('elu')(branch2)\n",
        "branch2 = keras.layers.AveragePooling2D((1,4))(branch2)\n",
        "branch2 = keras.layers.Dropout(0.5)(branch2)\n",
        "branch2 = keras.layers.SeparableConv2D(16, (1,16), padding='same', use_bias=False)(branch2)\n",
        "branch2 = keras.layers.BatchNormalization(axis=axVal)(branch2)\n",
        "branch2 = keras.layers.Activation('elu')(branch2)\n",
        "branch2 = keras.layers.AveragePooling2D((1,8))(branch2)\n",
        "branch2 = keras.layers.Dropout(0.5)(branch2)\n",
        "branch2 = keras.layers.Flatten()(branch2)\n",
        "\n",
        "# third branch\n",
        "input3 = keras.Input(shape=(input_shape))\n",
        "branch3 = keras.layers.Conv2D(16, (1, 256), padding='same', use_bias=False, input_shape=input_shape)(input3)\n",
        "branch3 = keras.layers.BatchNormalization(axis=axVal)(branch3)\n",
        "branch3 = keras.layers.DepthwiseConv2D((64,1), depth_multiplier=2, depthwise_constraint=keras.constraints.max_norm(1.), use_bias=False)(branch3)\n",
        "branch3 = keras.layers.BatchNormalization(axis=axVal)(branch3)\n",
        "branch3 = keras.layers.Activation('elu')(branch3)\n",
        "branch3 = keras.layers.AveragePooling2D((1,4))(branch3)\n",
        "branch3 = keras.layers.Dropout(0.5)(branch3)\n",
        "branch3 = keras.layers.SeparableConv2D(16, (1,32), padding='same', use_bias=False)(branch3)\n",
        "branch3 = keras.layers.BatchNormalization(axis=axVal)(branch3)\n",
        "branch3 = keras.layers.Activation('elu')(branch3)\n",
        "branch3 = keras.layers.AveragePooling2D((1,8))(branch3)\n",
        "branch3 = keras.layers.Dropout(0.5)(branch3)\n",
        "branch3 = keras.layers.Flatten()(branch3)\n",
        "\n",
        "# fusion\n",
        "merge1 = keras.layers.concatenate([branch1, branch2])\n",
        "merge2 = keras.layers.concatenate([merge1, branch3])\n",
        "\n",
        "flat = keras.layers.Flatten()(merge2)\n",
        "\n",
        "dense1 = keras.layers.Dense(2, kernel_constraint=keras.constraints.max_norm(0.25))(flat)\n",
        "\n",
        "mod_out = keras.layers.Softmax()(dense1)\n",
        "\n",
        "model = keras.Model(inputs=[input1, input2, input3], outputs=mod_out)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create callbacks for early stopping and adjusting learning rate\n",
        "\n",
        "callback_list = [\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.1, patience=10), keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    start_from_epoch=10\n",
        ")]"
      ],
      "metadata": {
        "id": "tkYAHWgCrazY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5qqnEgF8MdD"
      },
      "outputs": [],
      "source": [
        "# compile model\n",
        "model.summary()\n",
        "model.compile(loss = keras.losses.binary_crossentropy,optimizer= keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Training!\n",
        "hist = model.fit([x_train, x_train, x_train], y_train, batch_size=64, shuffle=True, epochs=300, validation_split=0.125, callbacks=callback_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "nbL4g-TgRE1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save training history\n",
        "np.save('/content/drive/MyDrive/DL Final/FusionWithFlipTrue.npy',hist.history)"
      ],
      "metadata": {
        "id": "AcYV5OFct0T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4pLyh4BD7uM"
      },
      "outputs": [],
      "source": [
        "# get test accuracy\n",
        "model.evaluate([x_test,x_test,x_test], y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No flip, fusion model - accuracy: 0.7718\n",
        "\n",
        "WIth flip, fusion model - accuracy: 0.8163\n"
      ],
      "metadata": {
        "id": "txVnWlf_3NLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model.save('/content/drive/MyDrive/DL Final/FusionWithFlipTrue.keras')"
      ],
      "metadata": {
        "id": "xrllSGPnCYIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}